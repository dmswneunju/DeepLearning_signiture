{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmswneunju/DeepLearning_signiture/blob/main/ch08_CNN_VGGNet_CIFAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VGGNet\n",
        "Convolutuin layer & pooling layer : feature 추출  \n",
        "Dense layer : 주어진 feature들이 어떤 카테고리에 속하는지 분류기 학습  \n",
        "VGGNet의 가장 큰 특징 : 모든 레이어가 3X3 kernel을 사용,  \n",
        "의의 : 커널 사이즈를 크게 설정하면 이미지의 넓은 영역에 대한 정보를 추출 가능.  But, 커널 사이즈 크게 하면 연산량이 많아짐. \n",
        "3X3 kernel은 작지만, convolution연산을 반복해서 수행하게 되면 커널사이즈를 키운 것과 동일한 효과 및 좋은 성능.  \n",
        "* 시각화 관련 모델이므로 torchvision에서 model import가능\n",
        "* loss : CrossEntropyLoss\n",
        "* optimizer : SGD  \n",
        "\n",
        "재활용하는 부분은 피쳐를 뽑아내는 부분. 주어진 이미지에서 피쳐를 잘 뽑아낼 수 있었기에 성능이 좋았음.  \n",
        "앞부분의 피쳐 뽑는부분은 그대로 fix. classifier에 대한부분은 다시 재학습.  \n",
        "즉, optimizer가 학습을 해야하는 부분도 classifier의 파라미터부분만 학습하도록 설정."
      ],
      "metadata": {
        "id": "eYAhunFxLlj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CIFAR10 Classifier(VGGNet)"
      ],
      "metadata": {
        "id": "-vldxeV2Pdzq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Step1] Load libraries & Datasets"
      ],
      "metadata": {
        "id": "gGTzdgb6P495"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZdTG9ypLdLO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import transforms #Augmentation\n",
        "from torchvision.transforms.functional import to_pil_image #데이터 시각화"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Step2] Data preprocessing\n",
        "불러온 이미지의 증강을 통해 학습 정확도를 향상\n",
        "* RandomCrop\n",
        "* RandomHorizontalFlip\n",
        "* Normalize -> 이미지 안을 구성하고있는 dataset에 대해서 평균과 표준편차를 맞춰주는 역할"
      ],
      "metadata": {
        "id": "7A7RlawNP5FB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 불러오기전에 transform까지 지정해주면 한번에 데이터전처리까지 진행해서 불러올 수 있다.\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)),\n",
        "])\n",
        "\n",
        " # CIFAR 데이터 불러오기\n",
        "train_img = datasets.CIFAR10(\n",
        "     root = 'data',\n",
        "     train = True,\n",
        "     download = True,\n",
        "     transform = transform\n",
        " )\n",
        "test_img = datasets.CIFAR10(\n",
        "     root = 'data',\n",
        "     train = False,\n",
        "     download = True,\n",
        "     transform = transform\n",
        " )"
      ],
      "metadata": {
        "id": "YyW9rml9ZQt6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6932971a-6a90-4627-807a-165aa585b28e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Step3] Set hyperparameters"
      ],
      "metadata": {
        "id": "8T9PITmZbEYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-3\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using Device:', DEVICE)"
      ],
      "metadata": {
        "id": "iuH9CNgma-TG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b635a23-0c75-4d1b-f131-f19ee4830d61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Step4] Create DataLoader"
      ],
      "metadata": {
        "id": "7zR3owvejRZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_img, batch_size = BATCH_SIZE, shuffle = True) \n",
        "test_loader = DataLoader(test_img, batch_size = BATCH_SIZE, shuffle = False)"
      ],
      "metadata": {
        "id": "sQF0aJycjiGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Step5] Set Network Structure"
      ],
      "metadata": {
        "id": "xFOAcmGsliHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 360),\n",
        "            nn.ReLU(inplace=True),#inplace 연산은 결과값을 새로운 변수에 값을 저장하는 대신 기존의 데이터를 대체\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(360, 100),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(100, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        return nn.Sequential(*layers)\n"
      ],
      "metadata": {
        "id": "EKxp4DjzlZzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Step6] Create Model instance"
      ],
      "metadata": {
        "id": "FAVWQwlZnlp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = VGG('VGG16').to(DEVICE)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wY_MEIp8nocn",
        "outputId": "5949fe26-02fe-49f6-bb8d-6259dca13d88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace=True)\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace=True)\n",
            "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace=True)\n",
            "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(inplace=True)\n",
            "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace=True)\n",
            "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=360, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=360, out_features=100, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=100, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Step7] Model compile"
      ],
      "metadata": {
        "id": "Vdqih4W7n0yW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loss\n",
        "loss = nn.CrossEntropyLoss() #다중 분류\n",
        "# Optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)"
      ],
      "metadata": {
        "id": "ip7RNv7AnzWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Step8] Set train loop"
      ],
      "metadata": {
        "id": "e5FS-pBYoDHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_loader, model, loss_fn, optimizer):\n",
        "    model.train()\n",
        "    \n",
        "    size = len(train_loader.dataset)\n",
        "    \n",
        "    for batch, (X, y) in enumerate(train_loader):\n",
        "        X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "        pred = model(X)\n",
        "\n",
        "        # 손실 계산\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # 역전파\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f'loss: {loss:>7f}  [{current:>5d}]/{size:5d}')"
      ],
      "metadata": {
        "id": "u6spfeyMoCGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Step9] Set test loop"
      ],
      "metadata": {
        "id": "OlgW8huWocqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(test_loader, model, loss_fn):\n",
        "    model.eval()\n",
        "\n",
        "    size = len(test_loader.dataset)\n",
        "    num_batches = len(test_loader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in test_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:8f}\\n\")"
      ],
      "metadata": {
        "id": "JcWDJiU1ocOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Step10] Run model"
      ],
      "metadata": {
        "id": "32zds4y4pW8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  print(f'EPOCHS {i+1} \\n------------------')\n",
        "  train(train_loader, model, loss, optimizer)\n",
        "  test(test_loader, model, loss)\n",
        "print('Done!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDoqxGIppTgs",
        "outputId": "2d616f54-df0d-4152-9ab9-f2b98a1707eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCHS 1 \n",
            "------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 2.330682  [    0]/50000\n",
            "loss: 1.991845  [ 3200]/50000\n",
            "loss: 2.123824  [ 6400]/50000\n",
            "loss: 1.852248  [ 9600]/50000\n",
            "loss: 1.694799  [12800]/50000\n",
            "loss: 1.503081  [16000]/50000\n",
            "loss: 1.806256  [19200]/50000\n",
            "loss: 1.576928  [22400]/50000\n",
            "loss: 1.736798  [25600]/50000\n",
            "loss: 1.568263  [28800]/50000\n",
            "loss: 1.635224  [32000]/50000\n",
            "loss: 1.576840  [35200]/50000\n",
            "loss: 1.360161  [38400]/50000\n",
            "loss: 1.026262  [41600]/50000\n",
            "loss: 0.993501  [44800]/50000\n",
            "loss: 1.260544  [48000]/50000\n",
            "Test Error: \n",
            " Accuracy: 51.8%, Avg loss: 1.283257\n",
            "\n",
            "EPOCHS 2 \n",
            "------------------\n",
            "loss: 1.740008  [    0]/50000\n",
            "loss: 1.005345  [ 3200]/50000\n",
            "loss: 1.169857  [ 6400]/50000\n",
            "loss: 1.574085  [ 9600]/50000\n",
            "loss: 1.142755  [12800]/50000\n",
            "loss: 1.110011  [16000]/50000\n",
            "loss: 0.973142  [19200]/50000\n",
            "loss: 1.197423  [22400]/50000\n",
            "loss: 1.049568  [25600]/50000\n",
            "loss: 1.116084  [28800]/50000\n",
            "loss: 1.420144  [32000]/50000\n",
            "loss: 0.853115  [35200]/50000\n",
            "loss: 0.873093  [38400]/50000\n",
            "loss: 0.838794  [41600]/50000\n",
            "loss: 0.849791  [44800]/50000\n",
            "loss: 0.797406  [48000]/50000\n",
            "Test Error: \n",
            " Accuracy: 69.0%, Avg loss: 0.882559\n",
            "\n",
            "EPOCHS 3 \n",
            "------------------\n",
            "loss: 0.988840  [    0]/50000\n",
            "loss: 1.156058  [ 3200]/50000\n",
            "loss: 0.999914  [ 6400]/50000\n",
            "loss: 0.926310  [ 9600]/50000\n",
            "loss: 1.065340  [12800]/50000\n",
            "loss: 0.966814  [16000]/50000\n",
            "loss: 0.922496  [19200]/50000\n",
            "loss: 0.593028  [22400]/50000\n",
            "loss: 0.929395  [25600]/50000\n",
            "loss: 0.779504  [28800]/50000\n",
            "loss: 0.845329  [32000]/50000\n",
            "loss: 1.091072  [35200]/50000\n",
            "loss: 0.537364  [38400]/50000\n",
            "loss: 1.147493  [41600]/50000\n",
            "loss: 1.355469  [44800]/50000\n",
            "loss: 0.909782  [48000]/50000\n",
            "Test Error: \n",
            " Accuracy: 72.7%, Avg loss: 0.798511\n",
            "\n",
            "EPOCHS 4 \n",
            "------------------\n",
            "loss: 0.926074  [    0]/50000\n",
            "loss: 1.118096  [ 3200]/50000\n",
            "loss: 0.639875  [ 6400]/50000\n",
            "loss: 0.630518  [ 9600]/50000\n",
            "loss: 0.612161  [12800]/50000\n",
            "loss: 1.351533  [16000]/50000\n",
            "loss: 0.916417  [19200]/50000\n",
            "loss: 0.724134  [22400]/50000\n",
            "loss: 0.675744  [25600]/50000\n",
            "loss: 0.992863  [28800]/50000\n",
            "loss: 0.767939  [32000]/50000\n",
            "loss: 1.049257  [35200]/50000\n",
            "loss: 0.877415  [38400]/50000\n",
            "loss: 0.612202  [41600]/50000\n",
            "loss: 0.867233  [44800]/50000\n",
            "loss: 0.863266  [48000]/50000\n",
            "Test Error: \n",
            " Accuracy: 75.2%, Avg loss: 0.737879\n",
            "\n",
            "EPOCHS 5 \n",
            "------------------\n",
            "loss: 1.002765  [    0]/50000\n",
            "loss: 0.687975  [ 3200]/50000\n",
            "loss: 0.980901  [ 6400]/50000\n",
            "loss: 0.929444  [ 9600]/50000\n",
            "loss: 0.531237  [12800]/50000\n",
            "loss: 0.907049  [16000]/50000\n",
            "loss: 0.712807  [19200]/50000\n",
            "loss: 0.681898  [22400]/50000\n",
            "loss: 0.570098  [25600]/50000\n",
            "loss: 0.404138  [28800]/50000\n",
            "loss: 1.130224  [32000]/50000\n",
            "loss: 0.403862  [35200]/50000\n",
            "loss: 0.559890  [38400]/50000\n",
            "loss: 0.790948  [41600]/50000\n",
            "loss: 0.425574  [44800]/50000\n",
            "loss: 0.536033  [48000]/50000\n",
            "Test Error: \n",
            " Accuracy: 79.5%, Avg loss: 0.618834\n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CIFAR Classifier(Pretrained VGGNet)\n",
        "ImageNet 데이터로 학습한 VGGNet을 사용하여 주어진 데이터 셋에서 사용할 수 있도록 Fine tuning"
      ],
      "metadata": {
        "id": "5Ck5GPqaNidV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "\n",
        "vgg16 = models.vgg16(pretrained=True)\n",
        "vgg16.to(DEVICE)\n",
        "print(vgg16)"
      ],
      "metadata": {
        "id": "fuU-YktXrw2K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f4f78b2-f7f0-482f-e079-4546f745b546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16.classifier[6].out_features=10\n",
        "\n",
        "for param in vgg16.features.parameters():\n",
        "  param.requires_grad=False #features를 구성하는 파라미터에 대해서 학습 진행 x"
      ],
      "metadata": {
        "id": "pJ4hVn13ODpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss\n",
        "loss = nn.CrossEntropyLoss() #다중 분류\n",
        "# Optimizer\n",
        "optimizer = torch.optim.SGD(vgg16.parameters(), lr=LEARNING_RATE, momentum=0.9)"
      ],
      "metadata": {
        "id": "papmuO0qOD0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  print(f'EPOCHS {i+1} \\n------------------')\n",
        "  train(train_loader, vgg16, loss, optimizer)\n",
        "  test(test_loader, vgg16, loss)\n",
        "print('Done!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucOvd4u_OD87",
        "outputId": "1df6f3ca-cefa-456e-cc29-a0a37218c5a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCHS 1 \n",
            "------------------\n",
            "loss: 13.333236  [    0]/50000\n",
            "loss: 1.046628  [ 3200]/50000\n",
            "loss: 1.079368  [ 6400]/50000\n",
            "loss: 0.819446  [ 9600]/50000\n",
            "loss: 0.378889  [12800]/50000\n",
            "loss: 0.811578  [16000]/50000\n",
            "loss: 1.032740  [19200]/50000\n",
            "loss: 0.579613  [22400]/50000\n",
            "loss: 0.679237  [25600]/50000\n",
            "loss: 0.586219  [28800]/50000\n",
            "loss: 0.671203  [32000]/50000\n",
            "loss: 0.503260  [35200]/50000\n",
            "loss: 0.378627  [38400]/50000\n",
            "loss: 0.760421  [41600]/50000\n",
            "loss: 0.565480  [44800]/50000\n",
            "loss: 0.810816  [48000]/50000\n",
            "Test Error: \n",
            " Accuracy: 82.8%, Avg loss: 0.494896\n",
            "\n",
            "EPOCHS 2 \n",
            "------------------\n",
            "loss: 0.795687  [    0]/50000\n",
            "loss: 0.539555  [ 3200]/50000\n",
            "loss: 0.309791  [ 6400]/50000\n",
            "loss: 0.833226  [ 9600]/50000\n",
            "loss: 0.431570  [12800]/50000\n",
            "loss: 0.423443  [16000]/50000\n",
            "loss: 0.430586  [19200]/50000\n",
            "loss: 0.859339  [22400]/50000\n",
            "loss: 0.288819  [25600]/50000\n",
            "loss: 0.404751  [28800]/50000\n",
            "loss: 0.378091  [32000]/50000\n",
            "loss: 0.236250  [35200]/50000\n",
            "loss: 0.271055  [38400]/50000\n",
            "loss: 0.864372  [41600]/50000\n",
            "loss: 0.436170  [44800]/50000\n",
            "loss: 0.493156  [48000]/50000\n",
            "Test Error: \n",
            " Accuracy: 84.5%, Avg loss: 0.448855\n",
            "\n",
            "EPOCHS 3 \n",
            "------------------\n",
            "loss: 0.373981  [    0]/50000\n",
            "loss: 0.651689  [ 3200]/50000\n",
            "loss: 0.252904  [ 6400]/50000\n",
            "loss: 0.230045  [ 9600]/50000\n",
            "loss: 0.237435  [12800]/50000\n",
            "loss: 0.314857  [16000]/50000\n",
            "loss: 0.282281  [19200]/50000\n",
            "loss: 0.295646  [22400]/50000\n",
            "loss: 0.465415  [25600]/50000\n",
            "loss: 0.280345  [28800]/50000\n",
            "loss: 0.294092  [32000]/50000\n",
            "loss: 0.276861  [35200]/50000\n",
            "loss: 0.232983  [38400]/50000\n",
            "loss: 0.515696  [41600]/50000\n",
            "loss: 0.380594  [44800]/50000\n",
            "loss: 0.235158  [48000]/50000\n",
            "Test Error: \n",
            " Accuracy: 85.9%, Avg loss: 0.414175\n",
            "\n",
            "EPOCHS 4 \n",
            "------------------\n",
            "loss: 0.519860  [    0]/50000\n",
            "loss: 0.249221  [ 3200]/50000\n",
            "loss: 0.436752  [ 6400]/50000\n",
            "loss: 0.447667  [ 9600]/50000\n",
            "loss: 0.151317  [12800]/50000\n",
            "loss: 0.160608  [16000]/50000\n",
            "loss: 0.109029  [19200]/50000\n",
            "loss: 0.244740  [22400]/50000\n",
            "loss: 0.195815  [25600]/50000\n",
            "loss: 0.276447  [28800]/50000\n",
            "loss: 0.393796  [32000]/50000\n",
            "loss: 0.600158  [35200]/50000\n",
            "loss: 0.346645  [38400]/50000\n",
            "loss: 0.336349  [41600]/50000\n",
            "loss: 0.308231  [44800]/50000\n",
            "loss: 0.362419  [48000]/50000\n",
            "Test Error: \n",
            " Accuracy: 86.4%, Avg loss: 0.404968\n",
            "\n",
            "EPOCHS 5 \n",
            "------------------\n",
            "loss: 0.436955  [    0]/50000\n",
            "loss: 0.259391  [ 3200]/50000\n",
            "loss: 0.513245  [ 6400]/50000\n",
            "loss: 0.403677  [ 9600]/50000\n",
            "loss: 0.209991  [12800]/50000\n",
            "loss: 0.228054  [16000]/50000\n",
            "loss: 0.264187  [19200]/50000\n",
            "loss: 0.190758  [22400]/50000\n",
            "loss: 0.227232  [25600]/50000\n",
            "loss: 0.184204  [28800]/50000\n",
            "loss: 0.258752  [32000]/50000\n",
            "loss: 0.158752  [35200]/50000\n",
            "loss: 0.494286  [38400]/50000\n",
            "loss: 0.283694  [41600]/50000\n",
            "loss: 0.095727  [44800]/50000\n",
            "loss: 0.248056  [48000]/50000\n",
            "Test Error: \n",
            " Accuracy: 86.8%, Avg loss: 0.400123\n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Og4d__b5W0R3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}